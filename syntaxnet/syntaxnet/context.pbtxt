Parameter {
  name: "brain_tokenizer_embedding_dims"
  value: "16;16;16"
}
Parameter {
  name: "brain_tokenizer_embedding_names"
  value: "chars;digits;puncts"
}
Parameter {
  name: "brain_tokenizer_features"
  value:  "input.char "
          "input(-1).char "
          "input(1).char; "
          "input.digit "
          "input(-1).digit "
          "input(1).digit; "
          "input.punctuation-amount "
          "input(-1).punctuation-amount "
          "input(1).punctuation-amount "
}
Parameter {
  name: "brain_tokenizer_transition_system"
  value: "binary-segment-transitions"
}
Parameter {
  name: "brain_morpher_embedding_dims"
  value: "2;16;8;16;16;16;16;16;64"
}
Parameter {
  name: "brain_morpher_embedding_names"
  value: "capitalization;char_ngram;other;prefix2;prefix3;suffix2;suffix3;tags;words"
}
Parameter {
  name: "brain_morpher_features"
  value: "input.capitalization "
         "input(1).capitalization "
         "input(2).capitalization "
         "input(3).capitalization "
         "input(-1).capitalization "
         "input(-2).capitalization "
         "input(-3).capitalization "
         "input(-4).capitalization; "
         "input.token.char-ngram "
         "input(1).token.char-ngram "
         "input(2).token.char-ngram "
         "input(3).token.char-ngram "
         "input(-1).token.char-ngram "
         "input(-2).token.char-ngram "
         "input(-3).token.char-ngram "
         "input(-4).token.char-ngram; "
         "input.digit "
         "input.hyphen "
         "input.token.punctuation-amount "
         "input.token.quote; "
         "input.token.prefix(length=2) "
         "input(1).token.prefix(length=2) "
         "input(2).token.prefix(length=2) "
         "input(3).token.prefix(length=2) "
         "input(-1).token.prefix(length=2) "
         "input(-2).token.prefix(length=2) "
         "input(-3).token.prefix(length=2) "
         "input(-4).token.prefix(length=2); "
         "input.token.prefix(length=3) "
         "input(1).token.prefix(length=3) "
         "input(2).token.prefix(length=3) "
         "input(3).token.prefix(length=3) "
         "input(-1).token.prefix(length=3) "
         "input(-2).token.prefix(length=3) "
         "input(-3).token.prefix(length=3) "
         "input(-4).token.prefix(length=3); "
         "input.token.suffix(length=2) "
         "input(1).token.suffix(length=2) "
         "input(2).token.suffix(length=2) "
         "input(3).token.suffix(length=2) "
         "input(-1).token.suffix(length=2) "
         "input(-2).token.suffix(length=2) "
         "input(-3).token.suffix(length=2) "
         "input(-4).token.suffix(length=2); "
         "input.token.suffix(length=3) "
         "input(1).token.suffix(length=3) "
         "input(2).token.suffix(length=3) "
         "input(3).token.suffix(length=3) "
         "input(-1).token.suffix(length=3) "
         "input(-2).token.suffix(length=3) "
         "input(-3).token.suffix(length=3) "
         "input(-4).token.suffix(length=3); "
         "input(-1).pred-morph-tag "
         "input(-2).pred-morph-tag "
         "input(-3).pred-morph-tag "
         "input(-4).pred-morph-tag; "
         "input.token.word "
         "input(1).token.word "
         "input(2).token.word "
         "input(3).token.word "
         "input(-1).token.word "
         "input(-2).token.word "
         "input(-3).token.word "
         "input(-4).token.word"
}
Parameter {
  name: "brain_morpher_transition_system"
  value: "morpher"
}
Parameter {
  name: 'brain_parser_embedding_dims'
  value: '64;32;32'
}
Parameter {
  name: 'brain_parser_features'
  value: 'input.word input(1).word input(2).word input(3).word stack.word stack(1).word stack(2).word stack(3).word stack.child(1).word stack.child(1).sibling(-1).word stack.child(-1).word stack.child(-1).sibling(1).word stack(1).child(1).word stack(1).child(1).sibling(-1).word stack(1).child(-1).word stack(1).child(-1).sibling(1).word stack.child(2).word stack.child(-2).word stack(1).child(2).word stack(1).child(-2).word;input.tag input(1).tag input(2).tag input(3).tag stack.tag stack(1).tag stack(2).tag stack(3).tag stack.child(1).tag stack.child(1).sibling(-1).tag stack.child(-1).tag stack.child(-1).sibling(1).tag stack(1).child(1).tag stack(1).child(1).sibling(-1).tag stack(1).child(-1).tag stack(1).child(-1).sibling(1).tag stack.child(2).tag stack.child(-2).tag stack(1).child(2).tag stack(1).child(-2).tag;stack.child(1).label stack.child(1).sibling(-1).label stack.child(-1).label stack.child(-1).sibling(1).label stack(1).child(1).label stack(1).child(1).sibling(-1).label stack(1).child(-1).label stack(1).child(-1).sibling(1).label stack.child(2).label stack.child(-2).label stack(1).child(2).label stack(1).child(-2).label'
}
Parameter {
  name: 'brain_parser_embedding_names'
  value: 'words;tags;labels'
}
Parameter {
  name: 'brain_parser_scoring'
  value: 'default'
}
Parameter {
  name: 'brain_pos_transition_system'
  value: 'tagger'
}
Parameter {
  name: 'brain_pos_embedding_dims'
  value: '64;4;8;8'
}
Parameter {
  name: 'brain_pos_features'
  value: 'stack(3).word stack(2).word stack(1).word stack.word input.word input(1).word input(2).word input(3).word;input.digit input.hyphen;stack.suffix(length=2) input.suffix(length=2) input(1).suffix(length=2);stack.prefix(length=2) input.prefix(length=2) input(1).prefix(length=2)'
}
Parameter {
  name: 'brain_pos_embedding_names'
  value: 'words;other;suffix;prefix'
}
input {
  name: 'training-corpus'
  record_format: 'conll-sentence'
  Part {
    file_pattern: 'syntaxnet/UD_Danish/treebank-train.trees.conll'
  }
}
input {
  name: 'tuning-corpus'
  record_format: 'conll-sentence'
  Part {
    file_pattern: 'syntaxnet/UD_Danish/dev.conll'
  }
}
input {
  name: 'dev-corpus'
  record_format: 'conll-sentence'
  Part {
    file_pattern: 'syntaxnet/UD_Danish/test.conll'
  }
}
input {
  name: 'morphed-training-corpus'
  creator: 'brain_morpher/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'morphed-tuning-corpus'
  creator: 'brain_morpher/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'morphed-dev-corpus'
  creator: 'brain_morpher/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'tagged-training-corpus'
  creator: 'brain_pos/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'tagged-tuning-corpus'
  creator: 'brain_pos/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'tagged-dev-corpus'
  creator: 'brain_pos/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'label-map'
  creator: 'brain_pos/greedy'
}
input {
  name: 'word-map'
  creator: 'brain_pos/greedy'
}
input {
  name: 'lcword-map'
  creator: 'brain_pos/greedy'
}
input {
  name: 'tag-map'
  creator: 'brain_pos/greedy'
}
input {
  name: 'category-map'
  creator: 'brain_pos/greedy'
}
input {
  name: 'char-map'
  creator: 'brain_pos/greedy'
}
input {
  name: 'prefix-table'
  creator: 'brain_pos/greedy'
}
input {
  name: 'suffix-table'
  creator: 'brain_pos/greedy'
}
input {
  name: 'tag-to-category'
  creator: 'brain_pos/greedy'
}
input {
  name: 'morph-label-set'
  creator: 'brain_pos/greedy'
}
input {
  name: 'morphology-map'
  creator: 'brain_pos/greedy'
}
input {
  name: 'char-ngram-map'
  creator: 'brain_pos/greedy'
}
input {
  name: 'label-map'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'word-map'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'lcword-map'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'tag-map'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'category-map'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'char-map'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'prefix-table'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'suffix-table'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'tag-to-category'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'morph-label-set'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'morphology-map'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'char-ngram-map'
  creator: 'brain_morpher/greedy'
}
input {
  name: 'projectivized-training-corpus'
  creator: 'brain_parser/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'parsed-training-corpus'
  creator: 'brain_parser/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'parsed-tuning-corpus'
  creator: 'brain_parser/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'parsed-dev-corpus'
  creator: 'brain_parser/greedy'
  record_format: 'conll-sentence'
}
input {
  name: 'beam-parsed-training-corpus'
  creator: 'brain_parser/structured'
  record_format: 'conll-sentence'
}
input {
  name: 'beam-parsed-tuning-corpus'
  creator: 'brain_parser/structured'
  record_format: 'conll-sentence'
}
input {
  name: 'beam-parsed-dev-corpus'
  creator: 'brain_parser/structured'
  record_format: 'conll-sentence'
}
input {
  name: 'stdin'
  record_format: 'english-text'
  Part {
    file_pattern: '-'
  }
}
input {
  name: 'stdin-conll'
  record_format: 'conll-sentence'
  Part {
    file_pattern: '-'
  }
}
input {
  name: 'stdout-conll'
  record_format: 'conll-sentence'
  Part {
    file_pattern: '-'
  }
}
Parameter {
  name: "brain_morpher_embedding_dims"
  value: "2;16;8;16;16;16;16;16;64"
}
Parameter {
  name: "brain_morpher_embedding_names"
  value: "capitalization;char_ngram;other;prefix2;prefix3;suffix2;suffix3;tags;words"
}
Parameter {
  name: "brain_morpher_features"
  value: "input.capitalization "
         "input(1).capitalization "
         "input(2).capitalization "
         "input(3).capitalization "
         "input(-1).capitalization "
         "input(-2).capitalization "
         "input(-3).capitalization "
         "input(-4).capitalization; "
         "input.token.char-ngram "
         "input(1).token.char-ngram "
         "input(2).token.char-ngram "
         "input(3).token.char-ngram "
         "input(-1).token.char-ngram "
         "input(-2).token.char-ngram "
         "input(-3).token.char-ngram "
         "input(-4).token.char-ngram; "
         "input.digit "
         "input.hyphen "
         "input.token.punctuation-amount "
         "input.token.quote; "
         "input.token.prefix(length=2) "
         "input(1).token.prefix(length=2) "
         "input(2).token.prefix(length=2) "
         "input(3).token.prefix(length=2) "
         "input(-1).token.prefix(length=2) "
         "input(-2).token.prefix(length=2) "
         "input(-3).token.prefix(length=2) "
         "input(-4).token.prefix(length=2); "
         "input.token.prefix(length=3) "
         "input(1).token.prefix(length=3) "
         "input(2).token.prefix(length=3) "
         "input(3).token.prefix(length=3) "
         "input(-1).token.prefix(length=3) "
         "input(-2).token.prefix(length=3) "
         "input(-3).token.prefix(length=3) "
         "input(-4).token.prefix(length=3); "
         "input.token.suffix(length=2) "
         "input(1).token.suffix(length=2) "
         "input(2).token.suffix(length=2) "
         "input(3).token.suffix(length=2) "
         "input(-1).token.suffix(length=2) "
         "input(-2).token.suffix(length=2) "
         "input(-3).token.suffix(length=2) "
         "input(-4).token.suffix(length=2); "
         "input.token.suffix(length=3) "
         "input(1).token.suffix(length=3) "
         "input(2).token.suffix(length=3) "
         "input(3).token.suffix(length=3) "
         "input(-1).token.suffix(length=3) "
         "input(-2).token.suffix(length=3) "
         "input(-3).token.suffix(length=3) "
         "input(-4).token.suffix(length=3); "
         "input(-1).pred-morph-tag "
         "input(-2).pred-morph-tag "
         "input(-3).pred-morph-tag "
         "input(-4).pred-morph-tag; "
         "input.token.word "
         "input(1).token.word "
         "input(2).token.word "
         "input(3).token.word "
         "input(-1).token.word "
         "input(-2).token.word "
         "input(-3).token.word "
         "input(-4).token.word"
}
Parameter {
  name: "brain_morpher_transition_system"
  value: "morpher"
}

